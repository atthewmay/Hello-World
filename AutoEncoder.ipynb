{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'IMAGES.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e568208b93d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0mvisualizeW1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt_W1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvis_patch_side\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhid_patch_side\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m \u001b[0mexecuteSparseAutoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-e568208b93d0>\u001b[0m in \u001b[0;36mexecuteSparseAutoencoder\u001b[0;34m()\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;34m\"\"\" Load randomly sampled image patches as dataset \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m     \u001b[0mtraining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_patches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvis_patch_side\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;34m\"\"\" Initialize the Autoencoder with the above parameters \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-e568208b93d0>\u001b[0m in \u001b[0;36mloadDataset\u001b[0;34m(num_patches, patch_side)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;34m\"\"\" Load images into numpy array \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'IMAGES.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'IMAGES'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/matthewhunt/anaconda/lib/python3.6/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \"\"\"\n\u001b[1;32m    134\u001b[0m     \u001b[0mvariable_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'variable_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m     \u001b[0mMR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m     \u001b[0mmatfile_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmdict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/matthewhunt/anaconda/lib/python3.6/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36mmat_reader_factory\u001b[0;34m(file_name, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m        \u001b[0mtype\u001b[0m \u001b[0mdetected\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \"\"\"\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mbyte_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0mmjv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmnv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_matfile_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmjv\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/matthewhunt/anaconda/lib/python3.6/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mappendmat\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'IMAGES.mat'"
     ]
    }
   ],
   "source": [
    "# This piece of software is bound by The MIT License (MIT)\n",
    "# Copyright (c) 2013 Siddharth Agrawal\n",
    "# Code written by : Siddharth Agrawal\n",
    "# Email ID : siddharth.950@gmail.com\n",
    "\n",
    "import numpy\n",
    "import math\n",
    "import time\n",
    "import scipy.io\n",
    "import scipy.optimize\n",
    "import matplotlib.pyplot\n",
    "\n",
    "###########################################################################################\n",
    "\"\"\" The Sparse Autoencoder class \"\"\"\n",
    "\n",
    "class SparseAutoencoder(object):\n",
    "\n",
    "    #######################################################################################\n",
    "    \"\"\" Initialization of Autoencoder object \"\"\"\n",
    "\n",
    "    def __init__(self, visible_size, hidden_size, rho, lamda, beta):\n",
    "    \n",
    "        \"\"\" Initialize parameters of the Autoencoder object \"\"\"\n",
    "    \n",
    "        self.visible_size = visible_size    # number of input units\n",
    "        self.hidden_size = hidden_size      # number of hidden units\n",
    "        self.rho = rho                      # desired average activation of hidden units\n",
    "        #note: this is that p term\n",
    "        self.lamda = lamda                  # weight decay parameter? \n",
    "        # I didn't realize they'd still include this regularization too.\n",
    "        self.beta = beta                    # weight of sparsity penalty term\n",
    "        \n",
    "        \"\"\" Set limits for accessing 'theta' values \"\"\"\n",
    "        # No idea what's going on here...\n",
    "        self.limit0 = 0\n",
    "        self.limit1 = hidden_size * visible_size\n",
    "        self.limit2 = 2 * hidden_size * visible_size\n",
    "        self.limit3 = 2 * hidden_size * visible_size + hidden_size\n",
    "        self.limit4 = 2 * hidden_size * visible_size + hidden_size + visible_size\n",
    "        \n",
    "        \"\"\" Initialize Neural Network weights randomly\n",
    "            W1, W2 values are chosen in the range [-r, r] \"\"\"\n",
    "        \n",
    "        r = math.sqrt(6) / math.sqrt(visible_size + hidden_size + 1) # I would have thought just the\n",
    "        # visible size in denominator for W1, becasue that's the number into each hidden node. I don't know\n",
    "        # Why the 6 is used. \n",
    "        \n",
    "        rand = numpy.random.RandomState(int(time.time())) # Clever\n",
    "        \n",
    "        W1 = numpy.asarray(rand.uniform(low = -r, high = r, size = (hidden_size, visible_size)))\n",
    "        W2 = numpy.asarray(rand.uniform(low = -r, high = r, size = (visible_size, hidden_size)))\n",
    "        \n",
    "        \"\"\" Bias values are initialized to zero \"\"\"\n",
    "        \n",
    "        b1 = numpy.zeros((hidden_size, 1))\n",
    "        b2 = numpy.zeros((visible_size, 1))\n",
    "\n",
    "        \"\"\" Create 'theta' by unrolling W1, W2, b1, b2 \"\"\"\n",
    "        #Question: why this?\n",
    "        self.theta = numpy.concatenate((W1.flatten(), W2.flatten(),\n",
    "                                        b1.flatten(), b2.flatten()))\n",
    "\n",
    "    #######################################################################################\n",
    "    \"\"\" Returns elementwise sigmoid output of input array \"\"\"\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "    \n",
    "        return (1 / (1 + numpy.exp(-x)))\n",
    "\n",
    "    #######################################################################################\n",
    "    \"\"\" Returns the cost of the Autoencoder and gradient at a particular 'theta' \"\"\"\n",
    "        \n",
    "    def sparseAutoencoderCost(self, theta, input):\n",
    "        \n",
    "        \"\"\" Extract weights and biases from 'theta' input \"\"\"\n",
    "        # why they are doing this... I don't know.\n",
    "        W1 = theta[self.limit0 : self.limit1].reshape(self.hidden_size, self.visible_size)\n",
    "        W2 = theta[self.limit1 : self.limit2].reshape(self.visible_size, self.hidden_size)\n",
    "        b1 = theta[self.limit2 : self.limit3].reshape(self.hidden_size, 1)\n",
    "        b2 = theta[self.limit3 : self.limit4].reshape(self.visible_size, 1)\n",
    "        \n",
    "        \"\"\" Compute output layers by performing a feedforward pass\n",
    "            Computation is done for all the training inputs simultaneously \"\"\"\n",
    "        # full-batch approach.\n",
    "        hidden_layer = self.sigmoid(numpy.dot(W1, input) + b1) # note that input examples are going to be in columns I think\n",
    "        # So the shape of hidden layer will be activations down the columns, with a new training eg per column\n",
    "        output_layer = self.sigmoid(numpy.dot(W2, hidden_layer) + b2)\n",
    "        \n",
    "        \"\"\" Estimate the average activation value of the hidden layers \"\"\"\n",
    "        \n",
    "        rho_cap = numpy.sum(hidden_layer, axis = 1) / input.shape[1]\n",
    "        \n",
    "        \"\"\" Compute intermediate difference values using Backpropagation algorithm \"\"\"\n",
    "        \n",
    "        diff = output_layer - input #It's like they are using a cross entropy cost. no.\n",
    "        \n",
    "        sum_of_squares_error = 0.5 * numpy.sum(numpy.multiply(diff, diff)) / input.shape[1] # cumulative measure.\n",
    "        weight_decay         = 0.5 * self.lamda * (numpy.sum(numpy.multiply(W1, W1)) +\n",
    "                                                   numpy.sum(numpy.multiply(W2, W2))) # It's element wise multiplication\n",
    "        # I don't understand what kind of regulatization they are doing here. It's not L2. or L1.\n",
    "        #It's a decay that changes based on sum of total weights.\n",
    "        \n",
    "        # Ah, the weight decay isn't the derivitive, but just the cost associated with the weights, the penalty\n",
    "        # Term.\n",
    "        \n",
    "        #rho_cap is the  average activation of that neuron in hidden layer\n",
    "        KL_divergence        = self.beta * numpy.sum(self.rho * numpy.log(self.rho / rho_cap) +\n",
    "                                                    (1 - self.rho) * numpy.log((1 - self.rho) / (1 - rho_cap)))\n",
    "        cost                 = sum_of_squares_error + weight_decay + KL_divergence\n",
    "        \n",
    "        KL_div_grad = self.beta * (-(self.rho / rho_cap) + ((1 - self.rho) / (1 - rho_cap)))\n",
    "        #I think this'll automatically assume the dimension of rho_cap. the # of hiddens is # rows, and 1 column\n",
    "        \n",
    "        # Question: What are they doing with this del_out? This isn't making much sense... \n",
    "        # Oh, of course! The derivative of the sigmoid function is the value of the sigmoid*(1-sigmoid)\n",
    "        # which means they are not using cross-entropy cost function, but rather the mse. I would think this would slow\n",
    "        #learning. Proper initialization would help with that tho.\n",
    "        del_out = numpy.multiply(diff, numpy.multiply(output_layer, 1 - output_layer))\n",
    "        \n",
    "        # structure: egs stored in each column. there are as many del_out as output neurons\n",
    "        \n",
    "        del_hid = numpy.multiply(numpy.dot(numpy.transpose(W2), del_out) + numpy.transpose(numpy.matrix(KL_div_grad)), \n",
    "                                 numpy.multiply(hidden_layer, 1 - hidden_layer)) # This last is the sigma*(1-sigma)\n",
    "        # think the addition of the np.matrix(KL_div_grad) gets applied to each example (column) in the np.dot().\n",
    "        \n",
    "        \n",
    "        \"\"\" Compute the gradient values by averaging partial derivatives\n",
    "            Partial derivatives are averaged over all training examples \"\"\"\n",
    "            \n",
    "        W1_grad = numpy.dot(del_hid, numpy.transpose(input)) # is [hiddens, examples] x [ examples, inputs]\n",
    "        # So you get W_11 = sum(all first hidden neuron*all first input), W_12 = sum(all first hidden * all second input)\n",
    "        W2_grad = numpy.dot(del_out, numpy.transpose(hidden_layer))\n",
    "        b1_grad = numpy.sum(del_hid, axis = 1)\n",
    "        b2_grad = numpy.sum(del_out, axis = 1)\n",
    "            \n",
    "        W1_grad = W1_grad / input.shape[1] + self.lamda * W1 # rescale the gradient to make it the average\n",
    "        # and implement the L2 regularization\n",
    "        W2_grad = W2_grad / input.shape[1] + self.lamda * W2\n",
    "        b1_grad = b1_grad / input.shape[1]\n",
    "        b2_grad = b2_grad / input.shape[1]\n",
    "        \n",
    "        \"\"\" Transform numpy matrices into arrays \"\"\"\n",
    "        \n",
    "        W1_grad = numpy.array(W1_grad)\n",
    "        W2_grad = numpy.array(W2_grad)\n",
    "        b1_grad = numpy.array(b1_grad)\n",
    "        b2_grad = numpy.array(b2_grad)\n",
    "        \n",
    "        \"\"\" Unroll the gradient values and return as 'theta' gradient \"\"\"\n",
    "        \n",
    "        theta_grad = numpy.concatenate((W1_grad.flatten(), W2_grad.flatten(),\n",
    "                                        b1_grad.flatten(), b2_grad.flatten()))\n",
    "                                        \n",
    "        return [cost, theta_grad]\n",
    "\n",
    "###########################################################################################\n",
    "\"\"\" Normalize the dataset provided as input \"\"\"\n",
    "\n",
    "def normalizeDataset(dataset):\n",
    "\n",
    "    \"\"\" Remove mean of dataset \"\"\"\n",
    "\n",
    "    dataset = dataset - numpy.mean(dataset)\n",
    "    \n",
    "    \"\"\" Truncate to +/-3 standard deviations and scale to -1 to 1 \"\"\"\n",
    "    \n",
    "    std_dev = 3 * numpy.std(dataset)\n",
    "    dataset = numpy.maximum(numpy.minimum(dataset, std_dev), -std_dev) / std_dev\n",
    "    \n",
    "    \"\"\" Rescale from [-1, 1] to [0.1, 0.9] \"\"\"\n",
    "    \n",
    "    dataset = (dataset + 1) * 0.4 + 0.1\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "###########################################################################################\n",
    "\"\"\" Randomly samples image patches, normalizes them and returns as dataset \"\"\"\n",
    "# It's cool that with this unsupervised stuff, you can\n",
    "def loadDataset(num_patches, patch_side):\n",
    "\n",
    "    \"\"\" Load images into numpy array \"\"\"\n",
    "\n",
    "    images = scipy.io.loadmat('IMAGES.mat')\n",
    "    images = images['IMAGES']\n",
    "    \n",
    "    \"\"\" Initialize dataset as array of zeros \"\"\"\n",
    "    \n",
    "    dataset = numpy.zeros((patch_side*patch_side, num_patches))\n",
    "    \n",
    "    \"\"\" Initialize random numbers for random sampling of images\n",
    "        There are 10 images of size 512 X 512 \"\"\"\n",
    "    \n",
    "    rand = numpy.random.RandomState(int(time.time()))\n",
    "    image_indices = rand.randint(512 - patch_side, size = (num_patches, 2))\n",
    "    image_number  = rand.randint(10, size = num_patches)\n",
    "    \n",
    "    \"\"\" Sample 'num_patches' random image patches \"\"\"\n",
    "    \n",
    "    for i in xrange(num_patches):\n",
    "    \n",
    "        \"\"\" Initialize indices for patch extraction \"\"\"\n",
    "    \n",
    "        index1 = image_indices[i, 0]\n",
    "        index2 = image_indices[i, 1]\n",
    "        index3 = image_number[i]\n",
    "        \n",
    "        \"\"\" Extract patch and store it as a column \"\"\"\n",
    "        \n",
    "        patch = images[index1:index1+patch_side, index2:index2+patch_side, index3]\n",
    "        patch = patch.flatten()\n",
    "        dataset[:, i] = patch\n",
    "    \n",
    "    \"\"\" Normalize and return the dataset \"\"\"\n",
    "    \n",
    "    dataset = normalizeDataset(dataset)\n",
    "    return dataset\n",
    "\n",
    "###########################################################################################\n",
    "\"\"\" Visualizes the obtained optimal W1 values as images \"\"\"\n",
    "\n",
    "def visualizeW1(opt_W1, vis_patch_side, hid_patch_side):\n",
    "\n",
    "    \"\"\" Add the weights as a matrix of images \"\"\"\n",
    "    \n",
    "    figure, axes = matplotlib.pyplot.subplots(nrows = hid_patch_side,\n",
    "                                              ncols = hid_patch_side)\n",
    "    index = 0\n",
    "                                              \n",
    "    for axis in axes.flat:\n",
    "    \n",
    "        \"\"\" Add row of weights as an image to the plot \"\"\"\n",
    "    \n",
    "        image = axis.imshow(opt_W1[index, :].reshape(vis_patch_side, vis_patch_side), # the opt_W1 is the same structure\n",
    "                            # as the W1. It's got all W1 going into a hidden node in a single row corresponding to that node.\n",
    "                            cmap = matplotlib.pyplot.cm.gray, interpolation = 'nearest')\n",
    "        axis.set_frame_on(False)\n",
    "        axis.set_axis_off()\n",
    "        index += 1\n",
    "        \n",
    "    \"\"\" Show the obtained plot \"\"\"  \n",
    "        \n",
    "    matplotlib.pyplot.show()\n",
    "\n",
    "###########################################################################################\n",
    "\"\"\" Loads data, trains the Autoencoder and visualizes the learned weights \"\"\"\n",
    "\n",
    "def executeSparseAutoencoder():\n",
    "\n",
    "    \"\"\" Define the parameters of the Autoencoder \"\"\"\n",
    "    \n",
    "    vis_patch_side = 8      # side length of sampled image patches\n",
    "    hid_patch_side = 5      # side length of representative image patches\n",
    "    rho            = 0.01   # desired average activation of hidden units\n",
    "    lamda          = 0.0001 # weight decay parameter\n",
    "    beta           = 3      # weight of sparsity penalty term\n",
    "    num_patches    = 10000  # number of training examples\n",
    "    max_iterations = 400    # number of optimization iterations\n",
    "\n",
    "    visible_size = vis_patch_side * vis_patch_side  # number of input units\n",
    "    hidden_size  = hid_patch_side * hid_patch_side  # number of hidden units\n",
    "    \n",
    "    \"\"\" Load randomly sampled image patches as dataset \"\"\"\n",
    "    \n",
    "    training_data = loadDataset(num_patches, vis_patch_side)\n",
    "    \n",
    "    \"\"\" Initialize the Autoencoder with the above parameters \"\"\"\n",
    "    \n",
    "    encoder = SparseAutoencoder(visible_size, hidden_size, rho, lamda, beta)\n",
    "    \n",
    "    \"\"\" Run the L-BFGS algorithm to get the optimal parameter values \"\"\"\n",
    "    # Woah, don't know that. But i think grad desc would also work. This must do somehting different\n",
    "    # with the gradient updates after the gradients are calculated. Rather than just the \n",
    "    # w = (1-decay)*w - eta*grad_w\n",
    "    opt_solution  = scipy.optimize.minimize(encoder.sparseAutoencoderCost, encoder.theta, \n",
    "                                            args = (training_data,), method = 'L-BFGS-B', \n",
    "                                            jac = True, options = {'maxiter': max_iterations})\n",
    "    opt_theta     = opt_solution.x\n",
    "    opt_W1        = opt_theta[encoder.limit0 : encoder.limit1].reshape(hidden_size, visible_size)\n",
    "    \n",
    "    \"\"\" Visualize the obtained optimal W1 weights \"\"\"\n",
    "    \n",
    "    visualizeW1(opt_W1, vis_patch_side, hid_patch_side)\n",
    "\n",
    "executeSparseAutoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4  9]\n",
      " [ 8 25]]\n"
     ]
    }
   ],
   "source": [
    "a = numpy.array([[2,9],[4,5]])\n",
    "b = numpy.array([[2,1],[2,5]])\n",
    "print(numpy.multiply(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This piece of software is bound by The MIT License (MIT)\n",
    "# Copyright (c) 2013 Siddharth Agrawal\n",
    "# Code written by : Siddharth Agrawal\n",
    "# Email ID : siddharth.950@gmail.com\n",
    "\n",
    "import numpy\n",
    "import math\n",
    "import time\n",
    "import scipy.io\n",
    "import scipy.optimize\n",
    "import matplotlib.pyplot\n",
    "\n",
    "###########################################################################################\n",
    "\"\"\" The Sparse Autoencoder class \"\"\"\n",
    "\n",
    "class SparseAutoencoder(object):\n",
    "\n",
    "    #######################################################################################\n",
    "    \"\"\" Initialization of Autoencoder object \"\"\"\n",
    "\n",
    "    def __init__(self, visible_size, hidden_size, rho, lamda, beta):\n",
    "    \n",
    "        \"\"\" Initialize parameters of the Autoencoder object \"\"\"\n",
    "    \n",
    "        self.visible_size = visible_size    # number of input units\n",
    "        self.hidden_size = hidden_size      # number of hidden units\n",
    "        self.rho = rho                      # desired average activation of hidden units\n",
    "        #note: this is that p term\n",
    "        self.lamda = lamda                  # weight decay parameter? \n",
    "        # I didn't realize they'd still include this regularization too.\n",
    "        self.beta = beta                    # weight of sparsity penalty term\n",
    "        \n",
    "        \"\"\" Set limits for accessing 'theta' values \"\"\"\n",
    "        # No idea what's going on here...\n",
    "        self.limit0 = 0\n",
    "        self.limit1 = hidden_size * visible_size\n",
    "        self.limit2 = 2 * hidden_size * visible_size\n",
    "        self.limit3 = 2 * hidden_size * visible_size + hidden_size\n",
    "        self.limit4 = 2 * hidden_size * visible_size + hidden_size + visible_size\n",
    "        \n",
    "        \"\"\" Initialize Neural Network weights randomly\n",
    "            W1, W2 values are chosen in the range [-r, r] \"\"\"\n",
    "        \n",
    "        r = math.sqrt(6) / math.sqrt(visible_size + hidden_size + 1) # I would have thought just the\n",
    "        # visible size in denominator for W1, becasue that's the number into each hidden node. I don't know\n",
    "        # Why the 6 is used. \n",
    "        \n",
    "        rand = numpy.random.RandomState(int(time.time())) # Clever\n",
    "        \n",
    "        W1 = numpy.asarray(rand.uniform(low = -r, high = r, size = (hidden_size, visible_size)))\n",
    "        W2 = numpy.asarray(rand.uniform(low = -r, high = r, size = (visible_size, hidden_size)))\n",
    "        \n",
    "        \"\"\" Bias values are initialized to zero \"\"\"\n",
    "        \n",
    "        b1 = numpy.zeros((hidden_size, 1))\n",
    "        b2 = numpy.zeros((visible_size, 1))\n",
    "\n",
    "        \"\"\" Create 'theta' by unrolling W1, W2, b1, b2 \"\"\"\n",
    "        #Question: why this?\n",
    "        self.theta = numpy.concatenate((W1.flatten(), W2.flatten(),\n",
    "                                        b1.flatten(), b2.flatten()))\n",
    "\n",
    "    #######################################################################################\n",
    "    \"\"\" Returns elementwise sigmoid output of input array \"\"\"\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "    \n",
    "        return (1 / (1 + numpy.exp(-x)))\n",
    "\n",
    "    #######################################################################################\n",
    "    \"\"\" Returns the cost of the Autoencoder and gradient at a particular 'theta' \"\"\"\n",
    "        \n",
    "    def sparseAutoencoderCost(self, theta, input):\n",
    "        \n",
    "        \"\"\" Extract weights and biases from 'theta' input \"\"\"\n",
    "        # why they are doing this... I don't know.\n",
    "        W1 = theta[self.limit0 : self.limit1].reshape(self.hidden_size, self.visible_size)\n",
    "        W2 = theta[self.limit1 : self.limit2].reshape(self.visible_size, self.hidden_size)\n",
    "        b1 = theta[self.limit2 : self.limit3].reshape(self.hidden_size, 1)\n",
    "        b2 = theta[self.limit3 : self.limit4].reshape(self.visible_size, 1)\n",
    "        \n",
    "        \"\"\" Compute output layers by performing a feedforward pass\n",
    "            Computation is done for all the training inputs simultaneously \"\"\"\n",
    "        # full-batch approach.\n",
    "        hidden_layer = self.sigmoid(numpy.dot(W1, input) + b1) # note that input examples are going to be in columns I think\n",
    "        # So the shape of hidden layer will be activations down the columns, with a new training eg per column\n",
    "        output_layer = self.sigmoid(numpy.dot(W2, hidden_layer) + b2)\n",
    "        \n",
    "        \"\"\" Estimate the average activation value of the hidden layers \"\"\"\n",
    "        \n",
    "        rho_cap = numpy.sum(hidden_layer, axis = 1) / input.shape[1]\n",
    "        \n",
    "        \"\"\" Compute intermediate difference values using Backpropagation algorithm \"\"\"\n",
    "        \n",
    "        diff = output_layer - input #It's like they are using a cross entropy cost. no.\n",
    "        \n",
    "        sum_of_squares_error = 0.5 * numpy.sum(numpy.multiply(diff, diff)) / input.shape[1] # cumulative measure.\n",
    "        weight_decay         = 0.5 * self.lamda * (numpy.sum(numpy.multiply(W1, W1)) +\n",
    "                                                   numpy.sum(numpy.multiply(W2, W2))) # It's element wise multiplication\n",
    "        # I don't understand what kind of regulatization they are doing here. It's not L2. or L1.\n",
    "        #It's a decay that changes based on sum of total weights.\n",
    "        \n",
    "        # Ah, the weight decay isn't the derivitive, but just the cost associated with the weights, the penalty\n",
    "        # Term.\n",
    "        \n",
    "        #rho_cap is the  average activation of that neuron in hidden layer\n",
    "        KL_divergence        = self.beta * numpy.sum(self.rho * numpy.log(self.rho / rho_cap) +\n",
    "                                                    (1 - self.rho) * numpy.log((1 - self.rho) / (1 - rho_cap)))\n",
    "        cost                 = sum_of_squares_error + weight_decay + KL_divergence\n",
    "        \n",
    "        KL_div_grad = self.beta * (-(self.rho / rho_cap) + ((1 - self.rho) / (1 - rho_cap)))\n",
    "        #I think this'll automatically assume the dimension of rho_cap. the # of hiddens is # rows, and 1 column\n",
    "        \n",
    "        # Question: What are they doing with this del_out? This isn't making much sense... \n",
    "        # Oh, of course! The derivative of the sigmoid function is the value of the sigmoid*(1-sigmoid)\n",
    "        # which means they are not using cross-entropy cost function, but rather the mse. I would think this would slow\n",
    "        #learning. Proper initialization would help with that tho.\n",
    "        del_out = numpy.multiply(diff, numpy.multiply(output_layer, 1 - output_layer))\n",
    "        \n",
    "        # structure: egs stored in each column. there are as many del_out as output neurons\n",
    "        \n",
    "        del_hid = numpy.multiply(numpy.dot(numpy.transpose(W2), del_out) + numpy.transpose(numpy.matrix(KL_div_grad)), \n",
    "                                 numpy.multiply(hidden_layer, 1 - hidden_layer)) # This last is the sigma*(1-sigma)\n",
    "        # think the addition of the np.matrix(KL_div_grad) gets applied to each example (column) in the np.dot().\n",
    "        \n",
    "        \n",
    "        \"\"\" Compute the gradient values by averaging partial derivatives\n",
    "            Partial derivatives are averaged over all training examples \"\"\"\n",
    "            \n",
    "        W1_grad = numpy.dot(del_hid, numpy.transpose(input)) # is [hiddens, examples] x [ examples, inputs]\n",
    "        # So you get W_11 = sum(all first hidden neuron*all first input), W_12 = sum(all first hidden * all second input)\n",
    "        W2_grad = numpy.dot(del_out, numpy.transpose(hidden_layer))\n",
    "        b1_grad = numpy.sum(del_hid, axis = 1)\n",
    "        b2_grad = numpy.sum(del_out, axis = 1)\n",
    "            \n",
    "        W1_grad = W1_grad / input.shape[1] + self.lamda * W1 # rescale the gradient to make it the average\n",
    "        # and implement the L2 regularization\n",
    "        W2_grad = W2_grad / input.shape[1] + self.lamda * W2\n",
    "        b1_grad = b1_grad / input.shape[1]\n",
    "        b2_grad = b2_grad / input.shape[1]\n",
    "        \n",
    "        \"\"\" Transform numpy matrices into arrays \"\"\"\n",
    "        \n",
    "        W1_grad = numpy.array(W1_grad)\n",
    "        W2_grad = numpy.array(W2_grad)\n",
    "        b1_grad = numpy.array(b1_grad)\n",
    "        b2_grad = numpy.array(b2_grad)\n",
    "        \n",
    "        \"\"\" Unroll the gradient values and return as 'theta' gradient \"\"\"\n",
    "        \n",
    "        theta_grad = numpy.concatenate((W1_grad.flatten(), W2_grad.flatten(),\n",
    "                                        b1_grad.flatten(), b2_grad.flatten()))\n",
    "                                        \n",
    "        return [cost, theta_grad]\n",
    "\n",
    "###########################################################################################\n",
    "\"\"\" Normalize the dataset provided as input \"\"\"\n",
    "\n",
    "def normalizeDataset(dataset):\n",
    "\n",
    "    \"\"\" Remove mean of dataset \"\"\"\n",
    "\n",
    "    dataset = dataset - numpy.mean(dataset)\n",
    "    \n",
    "    \"\"\" Truncate to +/-3 standard deviations and scale to -1 to 1 \"\"\"\n",
    "    \n",
    "    std_dev = 3 * numpy.std(dataset)\n",
    "    dataset = numpy.maximum(numpy.minimum(dataset, std_dev), -std_dev) / std_dev\n",
    "    \n",
    "    \"\"\" Rescale from [-1, 1] to [0.1, 0.9] \"\"\"\n",
    "    \n",
    "    dataset = (dataset + 1) * 0.4 + 0.1\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "###########################################################################################\n",
    "\"\"\" Randomly samples image patches, normalizes them and returns as dataset \"\"\"\n",
    "# It's cool that with this unsupervised stuff, you can\n",
    "def loadDataset(num_patches, patch_side):\n",
    "\n",
    "    \"\"\" Load images into numpy array \"\"\"\n",
    "\n",
    "    images = scipy.io.loadmat('IMAGES.mat')\n",
    "    images = images['IMAGES']\n",
    "    \n",
    "    \"\"\" Initialize dataset as array of zeros \"\"\"\n",
    "    \n",
    "    dataset = numpy.zeros((patch_side*patch_side, num_patches))\n",
    "    \n",
    "    \"\"\" Initialize random numbers for random sampling of images\n",
    "        There are 10 images of size 512 X 512 \"\"\"\n",
    "    \n",
    "    rand = numpy.random.RandomState(int(time.time()))\n",
    "    image_indices = rand.randint(512 - patch_side, size = (num_patches, 2))\n",
    "    image_number  = rand.randint(10, size = num_patches)\n",
    "    \n",
    "    \"\"\" Sample 'num_patches' random image patches \"\"\"\n",
    "    \n",
    "    for i in xrange(num_patches):\n",
    "    \n",
    "        \"\"\" Initialize indices for patch extraction \"\"\"\n",
    "    \n",
    "        index1 = image_indices[i, 0]\n",
    "        index2 = image_indices[i, 1]\n",
    "        index3 = image_number[i]\n",
    "        \n",
    "        \"\"\" Extract patch and store it as a column \"\"\"\n",
    "        \n",
    "        patch = images[index1:index1+patch_side, index2:index2+patch_side, index3]\n",
    "        patch = patch.flatten()\n",
    "        dataset[:, i] = patch\n",
    "    \n",
    "    \"\"\" Normalize and return the dataset \"\"\"\n",
    "    \n",
    "    dataset = normalizeDataset(dataset)\n",
    "    return dataset\n",
    "\n",
    "###########################################################################################\n",
    "\"\"\" Visualizes the obtained optimal W1 values as images \"\"\"\n",
    "\n",
    "def visualizeW1(opt_W1, vis_patch_side, hid_patch_side):\n",
    "\n",
    "    \"\"\" Add the weights as a matrix of images \"\"\"\n",
    "    \n",
    "    figure, axes = matplotlib.pyplot.subplots(nrows = hid_patch_side,\n",
    "                                              ncols = hid_patch_side)\n",
    "    index = 0\n",
    "                                              \n",
    "    for axis in axes.flat:\n",
    "    \n",
    "        \"\"\" Add row of weights as an image to the plot \"\"\"\n",
    "    \n",
    "        image = axis.imshow(opt_W1[index, :].reshape(vis_patch_side, vis_patch_side), # the opt_W1 is the same structure\n",
    "                            # as the W1. It's got all W1 going into a hidden node in a single row corresponding to that node.\n",
    "                            cmap = matplotlib.pyplot.cm.gray, interpolation = 'nearest')\n",
    "        axis.set_frame_on(False)\n",
    "        axis.set_axis_off()\n",
    "        index += 1\n",
    "        \n",
    "    \"\"\" Show the obtained plot \"\"\"  \n",
    "        \n",
    "    matplotlib.pyplot.show()\n",
    "\n",
    "###########################################################################################\n",
    "\"\"\" Loads data, trains the Autoencoder and visualizes the learned weights \"\"\"\n",
    "\n",
    "def executeSparseAutoencoder():\n",
    "\n",
    "    \"\"\" Define the parameters of the Autoencoder \"\"\"\n",
    "    \n",
    "    vis_patch_side = 8      # side length of sampled image patches\n",
    "    hid_patch_side = 5      # side length of representative image patches\n",
    "    rho            = 0.01   # desired average activation of hidden units\n",
    "    lamda          = 0.0001 # weight decay parameter\n",
    "    beta           = 3      # weight of sparsity penalty term\n",
    "    num_patches    = 10000  # number of training examples\n",
    "    max_iterations = 400    # number of optimization iterations\n",
    "\n",
    "    visible_size = vis_patch_side * vis_patch_side  # number of input units\n",
    "    hidden_size  = hid_patch_side * hid_patch_side  # number of hidden units\n",
    "    \n",
    "    \"\"\" Load randomly sampled image patches as dataset \"\"\"\n",
    "    \n",
    "    training_data = loadDataset(num_patches, vis_patch_side)\n",
    "    \n",
    "    \"\"\" Initialize the Autoencoder with the above parameters \"\"\"\n",
    "    \n",
    "    encoder = SparseAutoencoder(visible_size, hidden_size, rho, lamda, beta)\n",
    "    \n",
    "    \"\"\" Run the L-BFGS algorithm to get the optimal parameter values \"\"\"\n",
    "    # Woah, don't know that. But i think grad desc would also work. This must do somehting different\n",
    "    # with the gradient updates after the gradients are calculated. Rather than just the \n",
    "    # w = (1-decay)*w - eta*grad_w\n",
    "    opt_solution  = scipy.optimize.minimize(encoder.sparseAutoencoderCost, encoder.theta, \n",
    "                                            args = (training_data,), method = 'L-BFGS-B', \n",
    "                                            jac = True, options = {'maxiter': max_iterations})\n",
    "    opt_theta     = opt_solution.x\n",
    "    opt_W1        = opt_theta[encoder.limit0 : encoder.limit1].reshape(hidden_size, visible_size)\n",
    "    \n",
    "    \"\"\" Visualize the obtained optimal W1 weights \"\"\"\n",
    "    \n",
    "    visualizeW1(opt_W1, vis_patch_side, hid_patch_side)\n",
    "\n",
    "executeSparseAutoencoder()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
